---
title: "Practical machine Learning Course Project"
author: "Shounak Shastri"
date: "29/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible 
to collect a large amount of data about personal activity relatively 
inexpensively. These type of devices are part of the quantified self movement â€“ 
a group of enthusiasts who take measurements about themselves regularly to 
improve their health, to find patterns in their behavior, or because they are 
tech geeks. One thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it.

The goal of this project is to use data from accelerometers on the belt, 
forearm, arm, and dumbell of 6 participants. They were asked to perform barbell 
lifts correctly and incorrectly in 5 different ways. More information is 
available from the website here: 
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har
(see the section on the Weight Lifting Exercise Dataset) 


## Initial Set-up

### Downloading the Required Data

```{r}
setwd("C:\\Users\\Intel\\Documents\\Practical Machine Learning Course Project")

# Download Training Data
if(!file.exists("pml-training.csv"))
{
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                "pml-training.csv",
                method = 'curl')
}
trainingData <- read.csv("pml-training.csv", na.strings = c("NA", ""))

# Download Test Data
if(!file.exists("pml-testing.csv"))
{
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                "pml-testing.csv",
                method = 'curl')
}
testData <- read.csv("pml-testing.csv")
```

### Basic Data Exploration

We first check the size of the training data.

```{r }
dim(trainingData)
```

The training data has 160 columns which would result in 160 predictors. Let us 
check the summary of the columns to get an idea about what all the columns 
contain. This step can be helpful in deciding whether we can remove any columns 
from our data to speed up the training process.

```{r}
# Commented to save space as it displays the details of 160 columns. It is 
# advised to uncomment this line in order to check the data for NAs.

# str(trainingData)
```

### Data Cleaning

Now that we know what the data looks like, we can see that the first seven 
columns would not be useful in prediction. So we can remove the first 7 columns.

```{r}
trainingData <- trainingData[, -c(1:7)]
testData <- testData[, -c(1:7)]
dim(trainingData)
```

Removing columns with zero varience.

```{r}
library(caret)
NV <- nearZeroVar(trainingData)
trainingData <- trainingData[, -NV]
testData <- testData[, -NV]
dim(trainingData)
```

From the exploration step, we can see that some of the columns have NA values. 
We will check the data for columns which have lot of NA values and remove them.
This will make the analysis easier. Also we need to make sure that when we remove
any column from the training set, that column should be removed from the test 
set also. Otherwise it might result in wrong predictions or errors.

```{r}
colsWithNA <- sapply(trainingData, function(x) mean(is.na(x))) > 0.95
trainingData <- trainingData[, colsWithNA == FALSE]
testData <- testData[, colsWithNA == FALSE]

dim(trainingData)
```

Before splitting the training data into training and validation set, let us 
check the if we can further reduce the number of variables by applying PCA.
We will check the training set variables to see if they have a high correlation.

```{r}
library(corrplot)
corMatrix <- cor(trainingData[, -53])
```

It can be seen from the correlation plot shown in Appendix that very few columns
are display a high correlation. Therefore we can skip using PCA.

The correlation on the diagonal elements can be ignored as they 
represent the correlations with themselves. 

### Creating the Validation Set

Now, let us create a Validation set for the training data to validate the model.
The validation set can be created by partitioning 30% of our training data.

```{r}
set.seed(5) # For reproducability

inTrain  <- createDataPartition(trainingData$classe, p=0.7, list=FALSE)
trainingSet <- trainingData[inTrain, ]
validationSet  <- trainingData[-inTrain, ]
dim(trainingSet)
dim(validationSet)
```

Now we are ready to train models using our data.

## Model fitting

We are going to train a Random Forest model using the `trainingSet` and 
validate it on the `validationSet`. If the accuracy is low, then we can either
tune it or check a different model.

We are going to use the k-fold Cross Validation to tune our model with `k = 5`.
The reason for choosing `k = 5` can be noted here:

"...there is a bias-variance trade-off associated with the choice of k (...) 
one performs k-fold cross-validation using k = 5 or k = 10, as these values have
been shown empirically to yield test error rate estimates that suffer neither
from excessively high bias nor from very high variance." 

*- An Introduction to Statistical Learning, 2013*


```{r}
# library(randomForest)
control <- trainControl(method="cv", 5)
modelFit <- train(classe ~ .,
                 data=trainingSet,
                 method="rf",
                 trControl=control,
                 ntree=250)
modelFit
```

The model summary shows an accuracy of 0.99 (99%). This accuracy is high enough 
that we can use this model for validation and there is no need to train using a 
different modelling technique. 

Random Forest is a CPU intensive algorithm. I tried training the model with
different values of `ntree` and found 200 to be acceptable in terms of the time
required to train the model. `ntree = 175` results in an accuracy of
approximately 98.8%, which is quite high too. So for reducing the time and CPU 
loads, this value is also a valid choice. `ntree = 250` takes longer to
train and the accuracy is 99.08% which is a very small increase. So I have used 
`ntree = 200` as it gives an adequate accuracy.

Now we check the model on the Validation set

```{r}
modelValidation <- predict(modelFit, validationSet)
confusionMatrix(validationSet$classe, modelValidation)
```

The validation accuracy is 99.32% which is similar to the training accuracy.
This indicates that the model is not overfitted and can be used on the test data.

### Error Analysis

From the above confusion matrix and the statistics, we can check that the error
is low for all the classes as the Specificity values are mostly above 0.99.
This implies that the model is accurate and the validation set results are in 
line with the results from the training set.

### Evaluating the Test Data

```{r}
modelTest <- predict(modelFit, testData)
modelTest
```

## Appendix

The correlation Plot is as shown below

```{r}
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```